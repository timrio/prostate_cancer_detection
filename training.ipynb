{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Training and prediction notebook"]},{"cell_type":"markdown","metadata":{},"source":["This notebook containg everything to:\n","* define the Deep Learning model\n","* define the data sets and data loaders\n","* train the models\n","* make and save final predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","os.add_dll_directory(\"C:\\\\Users\\\\33631\\\\Desktop\\\\openslide-win64-20171122\\\\bin\")\n","\n","import openslide\n","from histolab.tiler import RandomTiler, GridTiler\n","from histolab.slide import Slide"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xYGeOQzf_Qyb"},"outputs":[],"source":["from PIL import Image\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import os\n","import random\n","import cv2\n","import matplotlib\n","from sklearn.model_selection import train_test_split\n","from skimage.filters import threshold_otsu\n","import torchmetrics\n","import pickle\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","import torchvision.models as models\n","import torchvision\n","from torchvision import transforms\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch import LongTensor as LongTensor\n","from torch import FloatTensor as FloatTensor\n","import cv2\n","from collections import Counter\n","from sklearn.model_selection import StratifiedKFold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lz_JA6j76vZn"},"outputs":[],"source":["if torch.cuda.is_available():\n","    device = 'cuda'\n","else:\n","    device = 'cpu'"]},{"cell_type":"markdown","metadata":{},"source":["## Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch_size = 16\n","number_of_tiles_per_bag = 10\n","number_of_folds = 10\n","n_epoch = 100\n","early_stopping = 10\n","learning_rate = 0.0001\n","embedding_shape = 1408"]},{"cell_type":"markdown","metadata":{"id":"y0tWUdTk_Qyo"},"source":["## Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBsSCIbf6yd8"},"outputs":[],"source":["train_set_path = \"./Data/raw_data/train.csv\"\n","train_tiles_encoding_folder = \"./Data/processed_data/train_tiles_encoding_grid/\"\n","\n","test_set_path = \"./Data/raw_data/test.csv\"\n","test_tiles_encoding_folder = \"./Data/processed_data/test_tiles_encoding_grid/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3zDZbWG_Qyp"},"outputs":[],"source":["initial_train_set = pd.read_csv(train_set_path)\n","test_set = pd.read_csv(test_set_path)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# in case we want to predict gleason score instead of ISUP grades\n","gleason_isup = initial_train_set[['isup_grade','gleason_score']].groupby('gleason_score').first()\n","gleason_isup_dict = gleason_isup.to_dict()['isup_grade']\n","\n","id2gleason = dict((k,v) for k,v in enumerate(list(initial_train_set.gleason_score.unique())))\n","gleason2id = dict((v,k) for k,v in id2gleason.items())\n","initial_train_set.gleason_score = initial_train_set.gleason_score.apply(lambda x: gleason2id[x])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOg7Zplk8JeH"},"outputs":[],"source":["## keep only images where enough tiles have been retrieved\n","to_remove = []\n","for idx in range(initial_train_set.shape[0]):\n","  value = initial_train_set.iloc[idx]\n","  image_id = value.image_id\n","  # compute file\n","  image_embeddings_path = train_tiles_encoding_folder+image_id+'.pkl'\n","\n","  # open embeddings  dict\n","  embeddings_dict = pickle.load(open(image_embeddings_path, 'rb'))\n","  if len(embeddings_dict) < number_of_tiles_per_bag:\n","    to_remove.append(image_id)\n","\n","initial_train_set = initial_train_set.query('image_id not in @to_remove')"]},{"cell_type":"markdown","metadata":{"id":"mFP-KmUS_Qyw"},"source":["## Cross Validations sets computation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kf = StratifiedKFold(n_splits=number_of_folds, random_state=None, shuffle=True)\n","folds_dict = {}\n","for i, (train_index, validation_index) in enumerate(kf.split(initial_train_set.drop(columns = ['isup_grade']), initial_train_set[['isup_grade']])):\n","    folds_dict[i] = (train_index, validation_index)"]},{"cell_type":"markdown","metadata":{"id":"n412JyPwOWNv"},"source":["## Define epoch training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgDHJwZ6OIyE"},"outputs":[],"source":["def train_one_epoch(model, trainloader, validationloader, optimizer, device, num_classes = 6): \n","    \"\"\"\n","    This function trains a model for one epoch\n","    inputs:\n","        - model: model to train \n","        - trainloader: loader for the training data\n","        - validationloader: loader for the validation data\n","        - optimizer: optimizer to use\n","        - device: cuda or cpu\n","        - num class: number of class to predict (6 for IUSP and 11 for gleason)\n","    \"\"\"\n","    losses = []\n","\n","    val_auroc = torchmetrics.AUROC(num_classes = num_classes)\n","    val_f1 = torchmetrics.F1Score(num_classes = num_classes)\n","    train_f1 = torchmetrics.F1Score(num_classes = num_classes)\n","    best_validation_f1 = -np.inf\n","\n","    ### traning \n","    model.train()\n","    for (features, target) in tqdm(trainloader):\n","        features, target = features.to(device), target.to(device)\n","\n","        optimizer.zero_grad()\n","        predictions = model(features)\n","        predicted_classes = torch.argmax(predictions, dim=1)\n","      \n","        criterion = nn.CrossEntropyLoss()\n","        loss = criterion(predictions, target)\n","        losses.append(float(loss))\n","        loss.backward()\n","        optimizer.step()\n","        f1_train = train_f1(predicted_classes.cpu(), target.cpu())\n","\n","    ### model evaluation \n","    model.eval()\n","    with torch.no_grad():\n","      for (features, target) in (validationloader):\n","          features, target = features.to(device), target.to(device)\n","\n","          predictions = model(features)\n","          predicted_classes = torch.argmax(predictions, dim=1)\n","          \n","          validation_auroc = val_auroc(predictions.cpu().detach(), target.cpu().detach())\n","          f1_val = val_f1(predicted_classes.cpu().detach(), target.cpu().detach())\n","\n","\n","    val_f1_final = val_f1.compute()    \n","    train_f1_final = train_f1.compute()  \n","    val_auroc_final = val_auroc.compute()\n","    print('average train loss: ', np.mean(losses))\n","    print('validation f1: ', val_f1_final)\n","    print('train f1: ', train_f1_final)\n","    print('validation auroc: ', val_auroc_final)\n","\n","    return val_auroc_final"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(model, trainloader, validationloader, optimizer, device, n_epoch, checkpoint_path, early_stopping, num_classes = 6):\n","    \"\"\"\n","    define the whole training of the model with early stopping strategy if the validation AUC stops increasing for a certain number \n","    of epochs\n","        inputs:\n","        - model: model to train \n","        - trainloader: loader for the training data\n","        - validationloader: loader for the validation data\n","        - optimizer: optimizer to use\n","        - device: cuda or cpu\n","        - n epochs: maximum number of epochs (if no early stopping)\n","        - checkpoint path: where to save the best models\n","        - early stopping: number of epochs after which early stopping is triggered\n","        - num class: number of class to predict (6 for IUSP and 11 for gleason)\n","    \"\"\"\n","    model.to(device)\n","\n","    \n","    best_roc = -np.inf\n","    previous_roc = -np.inf\n","    counter = 0\n","    for epoch in range(0, n_epoch):\n","        print(f\"epoch {epoch+1}/{n_epoch}\")\n","        roc = train_one_epoch(model , trainloader, validationloader, optimizer, device, num_classes = num_classes)\n","        if roc >= best_roc:\n","            best_roc = roc\n","            torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'roc': roc,\n","            }, checkpoint_path)\n","\n","            print('New best model saved !')\n","            counter = 0\n","\n","        else:\n","            print('no iprovement')\n","            counter += 1\n","\n","        if counter == early_stopping:\n","            print('early stopping !')\n","            break \n","    model.cpu() # remove model from gpu\n","    return\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Zp1Q_hLu_Qy6"},"source":["## Define data sets\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QiUQeJdJ_Qy7"},"outputs":[],"source":["class EmbeddingDatasetNoPosition(Dataset):\n","    \"\"\"\n","    data set that outputs bags and labels but no information about position of the tile\n","    \"\"\"\n","\n","    def __init__(self, df, dir, bag_size = 180, test = False):\n","        \"\"\"\n","        Inputs:\n","            df: test set or train set\n","            dir: where to find the tiles embeddings\n","            bag size: number of tiles per bag\n","            test: train or test set \n","        outputs:\n","            - bag content and label (if test = False)\n","        \"\"\"\n","        self.df = df\n","        self.dir = dir\n","        self.bag_size = bag_size\n","        self.test = test\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        value = self.df.iloc[idx]\n","        image_id = value.image_id\n","        # compute file\n","        image_embeddings_path = self.dir+image_id+'.pkl'\n","\n","        # open embeddings  dict\n","        embeddings_dict = pickle.load(open(image_embeddings_path, 'rb'))\n","\n","        bag = None\n","        embedding_list = np.array(list(embeddings_dict.values()))\n","        np.random.shuffle(embedding_list)\n","        bag = np.vstack(embedding_list[:self.bag_size])\n","\n","        bag = torch.Tensor(bag)\n","        # compute label\n","        if not self.test:\n","            label = torch.tensor(value.isup_grade)\n","            return bag, label\n","        else:\n","            return(bag)"]},{"cell_type":"markdown","metadata":{"id":"oPsx5A_cOa6x"},"source":["## Define Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKfLuUwmZli5"},"outputs":[],"source":["class ModelVanilla(nn.Module):\n","    \"\"\"\n","    baseline model\n","    \"\"\"\n","    def __init__(self, number_of_embeddings = 180, output_shape = 6, embedding_shape = 1280):\n","        super(ModelVanilla, self).__init__()\n","        \n","        # to encode each bag\n","        self.aggregator = torch.nn.AvgPool1d(number_of_embeddings)\n","\n","\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(embedding_shape, int(embedding_shape/2)),\n","                        nn.ReLU(),\n","                        torch.nn.BatchNorm1d(int(embedding_shape/2)),\n","                        nn.Dropout(0.25),\n","                        nn.Linear(int(embedding_shape/2), int(embedding_shape/4)),\n","                        nn.ReLU(),\n","                        nn.Dropout(0.25),\n","                        torch.nn.BatchNorm1d(int(embedding_shape/4)),\n","                        nn.Linear(int(embedding_shape/4), output_shape)\n","                        )\n","        \n","        \n","        self.softmax = nn.Softmax(dim = 1)\n","\n","\n","        \n","    def forward(self, bag):\n","      bag = torch.transpose(bag, 1, 2)\n","      wsi_descriptor = self.aggregator(bag).squeeze(-1)\n","      out = self.classifier(wsi_descriptor)\n","      out = self.softmax(out)\n","      return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gov3DEAmRg7K"},"outputs":[],"source":["class AttentionModel(nn.Module):\n","    \"\"\"\n","    initial attention model\n","    \"\"\"\n","    def __init__(self, number_of_embeddings = 180, output_shape = 6, embedding_shape = 1280):\n","        super(AttentionModel, self).__init__()\n","        \n","        # to compute attenation map for aggregation\n","        self.attention = nn.Sequential(\n","            nn.Linear(embedding_shape, int(embedding_shape/4)),\n","            nn.Tanh(),\n","            nn.Linear(int(embedding_shape/4), 1)\n","            )\n","\n","        self.aggregator = torch.nn.AvgPool1d(number_of_embeddings)\n","\n","\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(embedding_shape, int(embedding_shape/2)),\n","                        nn.ReLU(),\n","                        torch.nn.BatchNorm1d(int(embedding_shape/2)),\n","                        nn.Dropout(0.25),\n","                        nn.Linear(int(embedding_shape/2), int(embedding_shape/4)),\n","                        nn.ReLU(),\n","                        torch.nn.BatchNorm1d(int(embedding_shape/4)),\n","                        nn.Dropout(0.25),\n","                        nn.Linear(int(embedding_shape/4), output_shape)\n","        )\n","        \n","        \n","        self.softmax_attention = nn.Softmax(dim = 2)\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","\n","        \n","    def forward(self, bag):\n","      # compute attention map\n","      attention_map = self.attention(bag)\n","      attention_map = torch.transpose(attention_map, 2, 1) \n","      attention_map = self.softmax_attention(attention_map).squeeze(1)\n","\n","      # apply attention map\n","      attention_map = torch.diag_embed(attention_map)\n","      M = torch.bmm(attention_map, bag)\n","      M = torch.transpose(M, 2,1)\n","      # aggregate\n","      wsi_descriptor = self.aggregator(M).squeeze(-1)\n","\n","      # classification\n","      out = self.classifier(wsi_descriptor)\n","      out = self.softmax(out)\n","      return out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class GatedAttentionModel(nn.Module):\n","    \"\"\"\n","    model with gated attention\n","    \"\"\"\n","    def __init__(self, number_of_embeddings = 180, output_shape = 6, embedding_shape = 1280):\n","        super(GatedAttentionModel, self).__init__()\n","        \n","        # to compute attenation map for aggregation\n","        self.attention_tanh = nn.Sequential(\n","            nn.Linear(embedding_shape, int(embedding_shape/4)),\n","            nn.Tanh()\n","            )\n","\n","        self.attention_sig = nn.Sequential(\n","            nn.Linear(embedding_shape, int(embedding_shape/4)),\n","            nn.Sigmoid()\n","            )\n","\n","        self.attention_global = nn.Linear(int(embedding_shape/4), 1)\n","\n","        self.aggregator = torch.nn.AvgPool1d(number_of_embeddings)\n","\n","\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(embedding_shape, int(embedding_shape/2)),\n","                        nn.ReLU(),\n","                        torch.nn.BatchNorm1d(int(embedding_shape/2)),\n","                        nn.Dropout(0.25),\n","                        nn.Linear(int(embedding_shape/2), int(embedding_shape/4)),\n","                        nn.ReLU(),\n","                        torch.nn.BatchNorm1d(int(embedding_shape/4)),\n","                        nn.Dropout(0.25),\n","                        nn.Linear(int(embedding_shape/4), output_shape)\n","        )\n","        \n","        \n","        self.softmax_attention = nn.Softmax(dim = 2)\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","        \n","    def forward(self, bag):\n","      # compute attention map\n","      attention_sig = self.attention_sig(bag)\n","      attention_tan = self.attention_tanh(bag)\n","      attention_map = self.attention_global(torch.mul(attention_sig, attention_tan))\n","      \n","      attention_map = torch.transpose(attention_map, 2, 1) \n","      attention_map = self.softmax_attention(attention_map).squeeze(1)\n","\n","      # apply attention map\n","      attention_map = torch.diag_embed(attention_map)\n","      M = torch.bmm(attention_map, bag)\n","      M = torch.transpose(M, 2,1)\n","      # aggregate\n","      wsi_descriptor = self.aggregator(M).squeeze(-1)\n","\n","      # classification\n","      out = self.classifier(wsi_descriptor)\n","      out = self.softmax(out)\n","      return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXNpzoDZBBIg"},"outputs":[],"source":["class ModelChowder(nn.Module):\n","    \"\"\"\n","    Chowder Model\n","    \"\"\"\n","    def __init__(self, number_of_embeddings = 180, output_shape = 6, embedding_shape = 1280, R = 5):\n","        super(ModelChowder, self).__init__()\n","        \n","        # to encode each bag\n","        self.R = R\n","        self.conv = torch.nn.Conv1d(number_of_embeddings,number_of_embeddings,kernel_size =embedding_shape)\n","        \n","        self.classifier = nn.Sequential(\n","                        nn.Linear(2*self.R, 200),\n","                        nn.Sigmoid(),\n","                        torch.nn.BatchNorm1d(200),\n","                        nn.Dropout(0.25),\n","                        nn.Linear(200, 100),\n","                        nn.Sigmoid(),\n","                        torch.nn.BatchNorm1d(100),\n","                        nn.Dropout(0.25),\n","                        nn.Linear(100, output_shape)\n","                        )\n","        \n","        \n","        self.softmax = nn.Softmax(dim = 1)\n","\n","\n","        \n","    def forward(self, bag):\n","      post_conv = self.conv(bag).squeeze(-1)\n","      post_conv = torch.sort(post_conv)[0]\n","\n","      max_values = post_conv[:,-self.R:]\n","      min_values = post_conv[:,:self.R]\n","\n","      wsi_descriptor = torch.cat((max_values,min_values), 1)\n","\n","\n","\n","      out = self.classifier(wsi_descriptor)\n","      out = self.softmax(out)\n","      return out"]},{"cell_type":"markdown","metadata":{"id":"cAva6yVvOfTJ"},"source":["## Model training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["number_of_classes = 6"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# training loop for each model and each folds\n","for fold_number, (train_index, val_index) in folds_dict.items():\n","    print(f\"train fold: {fold_number}\")\n","\n","    # split train/val\n","    train_set, validation_set = initial_train_set.iloc[train_index], initial_train_set.iloc[val_index]\n","\n","    # create data sets\n","    train_set = EmbeddingDatasetNoPosition(train_set, train_tiles_encoding_folder, bag_size = number_of_tiles_per_bag)\n","    validation_set = EmbeddingDatasetNoPosition(validation_set, train_tiles_encoding_folder, bag_size = number_of_tiles_per_bag)\n","\n","    # create loaders\n","    trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last = True)\n","    validationloader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=True, drop_last = True)\n","\n","    # train models\n","    # Attention model\n","    \"\"\"\n","    print(\"start training Attention model\")\n","    model = AttentionModel(number_of_embeddings = number_of_tiles_per_bag, output_shape = number_of_classes, embedding_shape=embedding_shape)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate )\n","    checkpoint_path_attention = f\"./Data/checkpoints/model_attention_{fold_number+1}\"\n","    train_model(model, trainloader, validationloader, optimizer, device, n_epoch, checkpoint_path_attention, early_stopping, num_classes = number_of_classes)\n","    \"\"\"\n","\n","    print(\"start training Gated Attention model\")\n","    model = GatedAttentionModel(number_of_embeddings = number_of_tiles_per_bag, output_shape = number_of_classes, embedding_shape=embedding_shape)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate )\n","    checkpoint_path_attention = f\"./Data/checkpoints/model_gated_attention_{fold_number+1}\"\n","    train_model(model, trainloader, validationloader, optimizer, device, n_epoch, checkpoint_path_attention, early_stopping, num_classes = number_of_classes)\n","\n","    \"\"\"\n","    # Chowder model\n","    print(\"start training Chowder model\")\n","    model = ModelChowder(number_of_embeddings = number_of_tiles_per_bag, output_shape = number_of_classes,  embedding_shape=embedding_shape)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n","    checkpoint_path_chowder = f\"./Data/checkpoints/model_chowder_fold_{fold_number+1}\"\n","    train_model(model, trainloader, validationloader, optimizer, device, n_epoch, checkpoint_path_chowder, early_stopping, num_classes = number_of_classes)\n","\n","    # Vanilla model\n","    print(\"start training Vanilla model\")\n","    model = ModelVanilla(number_of_embeddings = number_of_tiles_per_bag, output_shape = number_of_classes,  embedding_shape=embedding_shape)\n","    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate )\n","    checkpoint_path_vanilla = f\"./Data/checkpoints/model_vanilla_{fold_number+1}\"\n","    best_model = train_model(model, trainloader, validationloader, optimizer, device, n_epoch, checkpoint_path_vanilla, early_stopping, num_classes = number_of_classes)\n","    \"\"\"\n"]},{"cell_type":"markdown","metadata":{"id":"iPbG-_uTJ8UB"},"source":["# Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","model_chowder = ModelChowder(number_of_embeddings = number_of_tiles_per_bag, output_shape = number_of_classes, embedding_shape=embedding_shape)\n","model_vanilla = ModelVanilla(number_of_embeddings = number_of_tiles_per_bag, output_shape = number_of_classes, embedding_shape=embedding_shape)\n","\"\"\"\n","model_attention_g = GatedAttentionModel(number_of_embeddings = number_of_tiles_per_bag, output_shape = number_of_classes, embedding_shape=embedding_shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# prediction loop for each model and each folds (100 predictions are made for each) --> argmax voting scheme\n","\n","\n","final_df = pd.DataFrame(columns = ['Id','Predicted'])\n","total_predictions = []\n","testing_set = EmbeddingDatasetNoPosition(test_set, test_tiles_encoding_folder, bag_size = number_of_tiles_per_bag, test = True)\n","testloader = torch.utils.data.DataLoader(testing_set, batch_size=len(test_set), shuffle=False)\n","for fold_number in range(number_of_folds):\n","    predictions_fold= []\n","    print(f\"current fold: {fold_number+1}\")\n","    # open checkpoints\n","    #checkpoint_chowder = torch.load(f\"./Data/checkpoints/model_chowder_fold_{fold_number+1}\")\n","    #checkpoint_vanilla = torch.load(f\"./Data/checkpoints/model_vanilla_{fold_number+1}\")\n","    checkpoint_attention_g = torch.load(f\"./Data/checkpoints/model_gated_attention_{fold_number+1}\")\n","\n","    # load weights\n","    #model_chowder.load_state_dict(checkpoint_chowder['model_state_dict']) \n","    #model_vanilla.load_state_dict(checkpoint_vanilla['model_state_dict']) \n","    model_attention_g.load_state_dict(checkpoint_attention_g['model_state_dict']) \n","    pred_fold = None\n","        \n","    for i in range(100):\n","        \n","        for bag in testloader:\n","\n","\n","\n","            # get predictions\n","            \"\"\"\n","            model_chowder.eval()\n","            pred_chowder = model_chowder(bag).detach().cpu().numpy()\n","\n","            model_vanilla.eval()\n","            pred_vanilla = model_vanilla(bag).detach().cpu().numpy()\n","            \"\"\"\n","\n","            model_attention_g.eval()\n","            pred_attention_g = model_attention_g(bag).detach().cpu().numpy()\n","\n","            if pred_fold is None:\n","                pred_fold = pred_attention_g #+ pred_vanilla  + pred_chowder\n","            else:\n","                pred_fold += pred_attention_g\n","\n","        pred_current_fold = list(np.argmax(pred_fold, axis =1))\n","\n","    total_predictions.append(pred_current_fold)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# max voiting between folds\n","\n","a = np.array(total_predictions)\n","final_prediction = []\n","for i in range(a.shape[1]):\n","    count_dict = Counter(a[:,i])\n","    pred  = max(count_dict, key=count_dict.get)\n","    final_prediction.append(pred)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create and save final data frame\n","final_df = pd.DataFrame()\n","final_df['Id'] = list(test_set.image_id)\n","final_df['Predicted'] = final_prediction\n","\n","final_df.to_csv(\"final_preds.csv\", index = False)"]},{"cell_type":"markdown","metadata":{},"source":["## Visualisation of attention map"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load a trained attention model\n","model_attention = AttentionModel(number_of_embeddings = number_of_tiles_per_bag, output_shape = number_of_classes, embedding_shape=embedding_shape)\n","checkpoint_attention = torch.load(f\"./Data/checkpoints/model_attention_{1}\")\n","model_attention.load_state_dict(checkpoint_attention['model_state_dict']) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class VisualisationSet(Dataset):\n","    \"\"\"\n","    Data set that outputs bags, label, positions of the tiles and images id\n","    \"\"\"\n","\n","    def __init__(self, df, dir, bag_size = 180, test = False):\n","        \"\"\"\n","        Inputs:\n","            df: test set or train set\n","            dir: where to find the tiles embeddings\n","            bag size: number of tiles per bag\n","            test: train or test set \n","        outputs:\n","            - bag content\n","            - tiles coordinates\n","            - label (ISUP grades)\n","            - image_id: id of the image whose tiles are in the bag\n","        \"\"\"\n","        self.df = df\n","        self.dir = dir\n","        self.bag_size = bag_size\n","        self.test = test\n","\n","    def __len__(self):\n","        return self.df.shape[0]\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        value = self.df.iloc[idx]\n","        image_id = value.image_id\n","        # compute file\n","        image_embeddings_path = self.dir+image_id+'.pkl'\n","\n","        # open embeddings  dict\n","        embeddings_dict = pickle.load(open(image_embeddings_path, 'rb'))\n","\n","        bag = None\n","        embedding_keys = list(embeddings_dict.keys())\n","        np.random.shuffle(embedding_keys)\n","        selected_keys = embedding_keys[:self.bag_size]\n","        bag = np.vstack([embeddings_dict[key] for key in list(selected_keys)])\n","        selected_keys = np.vstack(selected_keys) \n","        bag = torch.Tensor(bag)\n","        # compute label\n","        if not self.test:\n","            label = torch.tensor(value.isup_grade)\n","            return bag, selected_keys,image_id, label\n","        else:\n","            return(bag,selected_keys,image_id)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PlotModel(nn.Module):\n","    \"\"\"\n","    model that only outpus the attention map\n","    \"\"\"\n","    def __init__(self, attention_model):\n","        \"\"\"\n","        inputs:\n","            - attention model: trained attention model\n","        \"\"\"\n","        super(PlotModel, self).__init__()\n","        \n","        # to compute attenation map for aggregation\n","        self.attention = attention_model.attention\n","        \n","        self.softmax = nn.Softmax(dim = 2)\n","\n","\n","        \n","    def forward(self, bag):\n","      # compute attention map\n","      attention_map = self.attention(bag)\n","      attention_map = torch.transpose(attention_map, 2, 1) \n","      attention_map = self.softmax(attention_map).squeeze(1)\n","      return attention_map"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_set = VisualisationSet(initial_train_set, train_tiles_encoding_folder, bag_size = number_of_tiles_per_bag)\n","trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last = True)\n","plot_model = PlotModel(model_attention)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# get an example of attention map\n","attention = []\n","for emb,loc,id_list, lab in trainloader:\n","    break\n","\n","attention_values = plot_model(emb).detach().cpu().numpy()\n","id = id_list[0]\n","attention = attention_values[0]\n","locations = loc[0]\n","\n","# retrieve corresponding image and mask\n","\n","image = openslide.OpenSlide(\"Data/raw_data/train/train/\"+id+\".tiff\")\n","mask = openslide.OpenSlide(\"Data/raw_data/train_label_masks/train_label_masks/\"+id+\".tiff\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot mask\n","mask_array = np.array(mask.read_region((0,0), 0, mask.level_dimensions[0]))\n","cmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\n","plt.imshow(mask_array[:,:,0], cmap=cmap, interpolation='nearest', vmin=0, vmax=5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compute attention heat map\n","image_array = np.array(image.read_region((0,0), 0, image.level_dimensions[0]))\n","heat_map = np.zeros((image_array.shape[0], image_array.shape[1]))\n","for i,(x_ul_wsi,y_ul_wsi, x_br_wsi,y_br_wsi) in enumerate(locations):\n","    heat_map[y_ul_wsi:y_br_wsi,x_ul_wsi:x_br_wsi] = attention[i]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot the attention heat map on top on the WSI image\n","fig, ax = plt.subplots(figsize=(20, 16))\n","ax.imshow(image_array)\n","ax.imshow(heat_map, alpha = 0.5, cmap='jet' )"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Training_notebook.ipynb","version":""},"interpreter":{"hash":"cb07f93c22d8c46e8a2c451ad38041b75d10dd270b337033f28fdbf88ab40e30"},"kernelspec":{"display_name":"Python 3.8.10 ('new_env': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
